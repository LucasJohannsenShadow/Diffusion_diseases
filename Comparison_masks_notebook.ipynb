{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","machine_shape":"hm","authorship_tag":"ABX9TyPfYHyqFa8719AaMzsSV31Z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/disease_detection\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1DdEcnOIuElD","executionInfo":{"status":"ok","timestamp":1698921253535,"user_tz":-60,"elapsed":23280,"user":{"displayName":"CS Biostatistic PLEN","userId":"00637746287386755639"}},"outputId":"56b34579-a759-45f9-ac14-afcc01ac9e74"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/disease_detection\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"icG2-F5XqJd6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698327198913,"user_tz":-120,"elapsed":684,"user":{"displayName":"CS Biostatistic PLEN","userId":"00637746287386755639"}},"outputId":"3a3a6063-f2d2-4c48-b501-5021e9e6d0f1"},"outputs":[{"output_type":"stream","name":"stdout","text":["^C\n"]}],"source":["#!git clone https://github.com/lxfhfut/Self-Supervised-Leaf-Segmentation.git"]},{"cell_type":"code","source":["import torch\n","!pip install imutils\n","!pip install patchify\n","!pip install fastai\n","%cd Self_Supervised_Leaf_Segmentation"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sKKhPoJ7vQvj","executionInfo":{"status":"ok","timestamp":1698413995124,"user_tz":-120,"elapsed":16256,"user":{"displayName":"CS Biostatistic PLEN","userId":"00637746287386755639"}},"outputId":"5ec0afaf-b08f-43a3-9095-d15f0c800e62"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: imutils in /usr/local/lib/python3.10/dist-packages (0.5.4)\n","Collecting patchify\n","  Downloading patchify-0.2.3-py3-none-any.whl (6.6 kB)\n","Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from patchify) (1.23.5)\n","Installing collected packages: patchify\n","Successfully installed patchify-0.2.3\n","Requirement already satisfied: fastai in /usr/local/lib/python3.10/dist-packages (2.7.13)\n","Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (from fastai) (23.1.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fastai) (23.2)\n","Requirement already satisfied: fastdownload<2,>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from fastai) (0.0.7)\n","Requirement already satisfied: fastcore<1.6,>=1.5.29 in /usr/local/lib/python3.10/dist-packages (from fastai) (1.5.29)\n","Requirement already satisfied: torchvision>=0.11 in /usr/local/lib/python3.10/dist-packages (from fastai) (0.16.0+cu118)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from fastai) (3.7.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from fastai) (1.5.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fastai) (2.31.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from fastai) (6.0.1)\n","Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.10/dist-packages (from fastai) (1.0.3)\n","Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from fastai) (9.4.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from fastai) (1.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from fastai) (1.11.3)\n","Requirement already satisfied: spacy<4 in /usr/local/lib/python3.10/dist-packages (from fastai) (3.6.1)\n","Requirement already satisfied: torch<2.2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from fastai) (2.1.0+cu118)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (3.0.9)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (8.1.12)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (2.0.10)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (0.9.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (0.10.3)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (6.4.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (4.66.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (1.23.5)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (1.10.13)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (3.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (67.7.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (3.3.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fastai) (3.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fastai) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fastai) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fastai) (2023.7.22)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=1.10->fastai) (3.12.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=1.10->fastai) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=1.10->fastai) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=1.10->fastai) (3.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=1.10->fastai) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=1.10->fastai) (2.1.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fastai) (1.1.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fastai) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fastai) (4.43.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fastai) (1.4.5)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fastai) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fastai) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->fastai) (2023.3.post1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fastai) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fastai) (3.2.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->fastai) (1.16.0)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4->fastai) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4->fastai) (0.1.3)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<4->fastai) (8.1.7)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<4->fastai) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<2.2,>=1.10->fastai) (1.3.0)\n","/content/drive/MyDrive/disease_detection/Self_Supervised_Leaf_Segmentation\n"]}]},{"cell_type":"code","source":["!python leaf_segmenter.py --input ./../Data/data_masked/test/scap/2image.png #\\\n","                          #--save_video True\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6aL3uZFJvVqG","executionInfo":{"status":"ok","timestamp":1698406395247,"user_tz":-120,"elapsed":304674,"user":{"displayName":"CS Biostatistic PLEN","userId":"00637746287386755639"}},"outputId":"d57cfc80-ddec-43b8-dd5d-09b70cb89c0a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Iterations 199/200: 13, 3.20: 100% 200/200 [00:33<00:00,  5.90it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 18, 3.20: 100% 200/200 [00:31<00:00,  6.28it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 16, 3.20: 100% 200/200 [00:31<00:00,  6.30it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 8, 3.21: 100% 200/200 [00:31<00:00,  6.29it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 9, 3.20: 100% 200/200 [00:31<00:00,  6.30it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 13, 3.20: 100% 200/200 [00:31<00:00,  6.30it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 14, 3.20: 100% 200/200 [00:31<00:00,  6.30it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 11, 3.20: 100% 200/200 [00:31<00:00,  6.29it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 17, 3.20: 100% 200/200 [00:31<00:00,  6.31it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n"]}]},{"cell_type":"code","source":["def save_image(image, image_name, count, Method, specific_folder, category):\n","    \"\"\"Save an image to the specified path.\"\"\"\n","    cv2.imwrite('./../comparision_leaf_segmentation/'+ str(Method)+ '/' + str(specific_folder)+ '/' + str(category)+ '/' + str(count) + '_' + str(image_name)+ '.png', image)\n","import cv2\n","import numpy as np\n","\n","\n","def calculate_dice_coefficient(target, prediction):\n","    intersection = np.logical_and(target, prediction)\n","    dice = (2. * np.sum(intersection)) / (np.sum(target) + np.sum(prediction))\n","    return dice\n","\n","def color_difference(image_path1, groundtruth_path, dice_score_dict, count):\n","    # Read the images\n","    image1 = cv2.imread(image_path1, cv2.IMREAD_GRAYSCALE)  # Load in grayscale\n","    groundtruth = cv2.imread(groundtruth_path, cv2.IMREAD_GRAYSCALE)  # Load in grayscale\n","\n","    if image1 is None or groundtruth is None:\n","        raise ValueError(\"Could not read one of the images!\")\n","\n","    diff1 = cv2.subtract(image1, groundtruth)  # Areas unique to image1\n","    diff2 = cv2.subtract(groundtruth, image1)  # Areas unique to image2\n","\n","    # Find common areas\n","    common = cv2.bitwise_and(image1, groundtruth)\n","\n","    # Threshold the differences and common areas\n","    _, thresh_diff1 = cv2.threshold(diff1, 30, 255, cv2.THRESH_BINARY)\n","    _, thresh_diff2 = cv2.threshold(diff2, 30, 255, cv2.THRESH_BINARY)\n","    _, thresh_common = cv2.threshold(common, 30, 255, cv2.THRESH_BINARY)\n","\n","    # Prepare a color image for result\n","    superimposed = np.zeros((*image1.shape, 3), dtype=np.uint8)  # Same size as input, but 3 channels\n","\n","    # Set colors: unique parts of image1 as green, image2 as red, and overlap as white\n","    superimposed[thresh_diff1 == 255] = (0, 255, 0)  # green\n","    superimposed[thresh_diff2 == 255] = (0, 0, 255)  # red\n","    superimposed[thresh_common == 255] = (255, 255, 255)  # white\n","    #generating green and red versions\n","    lower_white = np.array([230])  # Slightly less than pure white to capture near white\n","    upper_white = np.array([255])\n","\n","    # Create masks to detect all white pixels\n","    white_pixels1 = cv2.inRange(image1, lower_white, upper_white)\n","    white_pixels2 = cv2.inRange(groundtruth, lower_white, upper_white)\n","\n","    groundtruth_green = np.zeros((*image1.shape, 3), dtype=np.uint8)  # Same size as input, but 3 channels\n","    superimposed2 = np.zeros((*image1.shape, 3), dtype=np.uint8)  # Same size as input, but 3 channels\n","\n","    # Change all white pixels to red\n","    groundtruth_green[white_pixels1 == 255] = [0, 255, 0]\n","    superimposed2[white_pixels2 == 255] = [0, 0, 255]\n","    image1 = image1/255.\n","    image1[image1 > 0.5] = 1\n","    image1[image1 <= 0.5] = 0\n","    groundtruth = groundtruth/255.\n","    groundtruth[groundtruth > 0.5] = 1\n","    groundtruth[groundtruth <= 0.5] = 0\n","\n","    #_, image1 = cv2.threshold(image1, 127, 255, cv2.THRESH_BINARY)\n","    #image1 = np.where(image1 == 255, 1, 0)\n","    #_, image2 = cv2.threshold(image2, 127, 255, cv2.THRESH_BINARY)\n","    #image2 = np.where(image2 == 255, 1, 0)\n","    dice_score_dict[count] = calculate_dice_coefficient(groundtruth, image1)\n","\n","    return groundtruth_green, superimposed\n","#Replace 'path_to_image1.jpg' and 'path_to_image2.jpg' with your actual file paths\n","def masking_OpenCV(img_path):\n","        img = cv2.imread(img_path)\n","        lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)    # Change the colorspace\n","        a_channel = lab[:, :, 1]                        # Select green color channel\n","        th = cv2.threshold(a_channel, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1] # Threshold with OTSU\n","        th = cv2.dilate(th, None, iterations=7) # Bring Contours togesther\n","        th = cv2.erode(th, None, iterations=8)\n","        th = th.astype(np.uint8)\n","        #cv2.imwrite('./results_all/'+'data_masked'+'/'+str(count)+'_'+'th.png', th)\n","        contours = cv2.findContours(th, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","        contours = imutils.grab_contours(contours)\n","        # Create a mask to fill the area around the contours\n","        mask = img.copy()\n","        for cnt in contours:\n","            cv2.fillPoly(mask, [cnt]\n","                        , (0, 0, 0))  # Fill the area around the contours with black\n","        # Create an inverse mask to keep the original content within the contours\n","        inverse_mask = cv2.bitwise_not(mask)\n","        # Combine the original image with the filled area using the inverse mask\n","        img = cv2.bitwise_and(img, inverse_mask)\n","        return th, img\n","\n","\n","import re\n","\n","def atoi(text):\n","    return int(text) if text.isdigit() else text\n","\n","def natural_keys(text):\n","    '''\n","    A list of integer or text chunks to be used as keys for sorting.\n","    \"z23.txt\" -> [\"z\", 23, \".txt\"]\n","    '''\n","    return [ atoi(c) for c in re.split(r'(\\d+)', text)]"],"metadata":{"id":"SRf5UVM_2Z6_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import imutils\n","#generating mask with openCV\n","input_folder = \"./../Data/data_masked/test/good/\"\n","input_files = os.listdir(input_folder)\n","input_files.sort(key=natural_keys)\n","count =  0\n","for filename in input_files:\n","  if filename.endswith(('.png')):\n","    print(filename)\n","    pth_to_img = os.path.join(input_folder, filename)\n","    th, _ =  masking_2(pth_to_img)\n","    save_image(image = th , image_name = 'th', count = count, Method= 'OpenCV', specific_folder= 'generated_masks',category='good')\n","    count += 1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":245},"id":"G1wRUCMx2nXX","executionInfo":{"status":"error","timestamp":1698403971253,"user_tz":-120,"elapsed":258,"user":{"displayName":"CS Biostatistic PLEN","userId":"00637746287386755639"}},"outputId":"55e10edc-beea-4b07-c654-6ce286697077"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-46d358be8d35>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0minput_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./../Data/data_masked/test/good/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minput_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0minput_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnatural_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'natural_keys' is not defined"]}]},{"cell_type":"code","source":["# @title Comparision OpenCV\n","import pandas as pd\n","import os\n","import imutils\n","#generating mask with openCV\n","input_folder = \"./../Data/data_masked/test/good/\"\n","input_files = os.listdir(input_folder)\n","input_files.sort(key=natural_keys)\n","count =  0\n","for filename in input_files:\n","  if filename.endswith(('.png')):\n","    print(filename)\n","    pth_to_img = os.path.join(input_folder, filename)\n","    th, _ =  masking_OpenCV(pth_to_img)\n","    save_image(image = th , image_name = 'th_good', count = count, Method= 'OpenCV', specific_folder= 'generated_masks',category='good')\n","    count += 1\n","#generating mask with openCV\n","input_folder = \"./../Data/data_masked/test/scap/\"\n","input_files = os.listdir(input_folder)\n","input_files.sort(key=natural_keys)\n","count =  0\n","for filename in input_files:\n","  if filename.endswith(('.png')):\n","    print(filename)\n","    pth_to_img = os.path.join(input_folder, filename)\n","    th, _ =  masking_OpenCV(pth_to_img)\n","    save_image(image = th , image_name = 'th_scap', count = count, Method= 'OpenCV', specific_folder= 'generated_masks',category='scap')\n","    count += 1\n","#generating comparisions between groundtruth and OpenCV for good images\n","input_folder_openCV = \"./../comparision_leaf_segmentation/OpenCV/generated_masks/good/\"\n","input_files_openCV = os.listdir(input_folder_openCV)\n","input_files_openCV.sort(key=natural_keys)\n","\n","input_folder_groundtruth = \"./../Data/data_masked/ground_truth_leaf/good/\"\n","input_files_groundtruth = os.listdir(input_folder_groundtruth)\n","input_files_groundtruth.sort(key=natural_keys)\n","dice_score_dict, iou_score_dict = {},{}\n","count =  0\n","for file_openCV, file_groundtruth in zip(input_files_openCV, input_files_groundtruth):\n","    print(f\"Processing {file_openCV} and {file_groundtruth}\")  # Check if loop is entered\n","\n","    pth_to_file_openCV = os.path.join(input_folder_openCV, file_openCV)\n","    pth_to_file_groundtruth = os.path.join(input_folder_groundtruth, file_groundtruth)\n","    groundtruth_green, superimposed  = color_difference(pth_to_file_openCV, pth_to_file_groundtruth, dice_score_dict,  count)\n","    save_image(image = superimposed , image_name = 'superimposed_OpenCV_good', count = count, Method= 'OpenCV', specific_folder= 'comparision_files',category='good')\n","    save_image(image = groundtruth_green , image_name = 'groundtruth_green_good', count = count, Method= 'green_ground_truth', specific_folder= 'specific_folder', category='good')\n","    count += 1\n","dice_score_dict = pd.DataFrame(dice_score_dict.values(), columns=['dice_score'], index=dice_score_dict.keys())\n","with open(\"./../comparision_leaf_segmentation/OpenCV/comparision_files/good/dice_score_OpenCV_good.txt\", \"w\") as f:\n","             f.write(dice_score_dict.to_string())\n","#generating comparisions between groundtruth and OpenCV for scap images\n","input_folder_openCV = \"./../comparision_leaf_segmentation/OpenCV/generated_masks/scap/\"\n","input_files_openCV = os.listdir(input_folder_openCV)\n","input_files_openCV.sort(key=natural_keys)\n","\n","input_folder_groundtruth = \"./../Data/data_masked/ground_truth_leaf/scap/\"\n","input_files_groundtruth = os.listdir(input_folder_groundtruth)\n","input_files_groundtruth.sort(key=natural_keys)\n","dice_score_dict, iou_score_dict = {},{}\n","count =  0\n","for file_openCV, file_groundtruth in zip(input_files_openCV, input_files_groundtruth):\n","    print(f\"Processing {file_openCV} and {file_groundtruth}\")  # Check if loop is entered\n","\n","    pth_to_file_openCV = os.path.join(input_folder_openCV, file_openCV)\n","    pth_to_file_groundtruth = os.path.join(input_folder_groundtruth, file_groundtruth)\n","    groundtruth_green, superimposed  = color_difference(pth_to_file_openCV, pth_to_file_groundtruth, dice_score_dict,  count)\n","    save_image(image = superimposed , image_name = 'superimposed_OpenCV_scap', count = count, Method= 'OpenCV', specific_folder= 'comparision_files',category='scap')\n","    save_image(image = groundtruth_green , image_name = 'groundtruth_green_scap', count = count, Method= 'green_ground_truth', specific_folder= 'specific_folder', category='scap')\n","    count += 1\n","dice_score_dict = pd.DataFrame(dice_score_dict.values(), columns=['dice_score'], index=dice_score_dict.keys())\n","with open(\"./../comparision_leaf_segmentation/OpenCV/comparision_files/scap/dice_score_OpenCV_scap.txt\", \"w\") as f:\n","             f.write(dice_score_dict.to_string())\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ynwhO0-b2o0u","executionInfo":{"status":"ok","timestamp":1698414652096,"user_tz":-120,"elapsed":20508,"user":{"displayName":"CS Biostatistic PLEN","userId":"00637746287386755639"}},"outputId":"12752a9d-360d-4759-c9c9-d954b51ac412"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0image.png\n","1image.png\n","2image.png\n","3image.png\n","4image.png\n","5image.png\n","6image.png\n","7image.png\n","8image.png\n","0image.png\n","1image.png\n","2image.png\n","3image.png\n","4image.png\n","5image.png\n","6image.png\n","7image.png\n","8image.png\n","9image.png\n","10image.png\n","11image.png\n","12image.png\n","13image.png\n","14image.png\n","15image.png\n","16image.png\n","17image.png\n","18image.png\n","19image.png\n","20image.png\n","21image.png\n","22image.png\n","23image.png\n","24image.png\n","25image.png\n","26image.png\n","27image.png\n","28image.png\n","29image.png\n","30image.png\n","31image.png\n","32image.png\n","Processing 0_th_good.png and 0leaf_mask.png\n","Processing 1_th_good.png and 1leaf_mask.png\n","Processing 2_th_good.png and 2leaf_mask.png\n","Processing 3_th_good.png and 3leaf_mask.png\n","Processing 4_th_good.png and 4leaf_mask.png\n","Processing 5_th_good.png and 5leaf_mask.png\n","Processing 6_th_good.png and 6leaf_mask.png\n","Processing 7_th_good.png and 7leaf_mask.png\n","Processing 8_th_good.png and 8leaf_mask.png\n","Processing 0_th_scap.png and 0leaf_mask.png\n","Processing 1_th_scap.png and 1leaf_mask.png\n","Processing 2_th_scap.png and 2leaf_mask.png\n","Processing 3_th_scap.png and 3leaf_mask.png\n","Processing 4_th_scap.png and 4leaf_mask.png\n","Processing 5_th_scap.png and 5leaf_mask.png\n","Processing 6_th_scap.png and 6leaf_mask.png\n","Processing 7_th_scap.png and 7leaf_mask.png\n","Processing 8_th_scap.png and 8leaf_mask.png\n","Processing 9_th_scap.png and 9leaf_mask.png\n","Processing 10_th_scap.png and 10leaf_mask.png\n","Processing 11_th_scap.png and 11leaf_mask.png\n","Processing 12_th_scap.png and 12leaf_mask.png\n","Processing 13_th_scap.png and 13leaf_mask.png\n","Processing 14_th_scap.png and 14leaf_mask.png\n","Processing 15_th_scap.png and 15leaf_mask.png\n","Processing 16_th_scap.png and 16leaf_mask.png\n","Processing 17_th_scap.png and 17leaf_mask.png\n","Processing 18_th_scap.png and 18leaf_mask.png\n","Processing 19_th_scap.png and 19leaf_mask.png\n","Processing 20_th_scap.png and 20leaf_mask.png\n","Processing 21_th_scap.png and 21leaf_mask.png\n","Processing 22_th_scap.png and 22leaf_mask.png\n","Processing 23_th_scap.png and 23leaf_mask.png\n","Processing 24_th_scap.png and 24leaf_mask.png\n","Processing 25_th_scap.png and 25leaf_mask.png\n","Processing 26_th_scap.png and 26leaf_mask.png\n","Processing 27_th_scap.png and 27leaf_mask.png\n","Processing 28_th_scap.png and 28leaf_mask.png\n","Processing 29_th_scap.png and 29leaf_mask.png\n","Processing 30_th_scap.png and 30leaf_mask.png\n","Processing 31_th_scap.png and 31leaf_mask.png\n","Processing 32_th_scap.png and 32leaf_mask.png\n"]}]},{"cell_type":"code","source":["# @title Comparision Instand Segmentation\n","import pandas as pd\n","import os\n","import imutils\n","#generating mask with Instand Segmentation\n","!python leaf_segmenter.py --input ./../Data/data_masked/test/scap/2image.png                                                     --input_folder ./../Data/data_masked/test/good/ \\\n","                          --input_folder ./../Data/data_masked/test/good/ \\\n","                          --comparison_folder ./../comparision_leaf_segmentation/Self_supervised_segmentation/generated_masks/good/\n","!python leaf_segmenter.py --input ./../Data/data_masked/test/scap/2image.png \\\n","                          --input_folder ./../Data/data_masked/test/scap/ \\\n","                          --comparison_folder ./../comparision_leaf_segmentation/Self_supervised_segmentation/generated_masks/scap/\n","\n","#generating comparisions between groundtruth and OpenCV for good images\n","input_folder_openCV = \"./../comparision_leaf_segmentation/Self_supervised_segmentation/generated_masks/good/\"\n","input_files_openCV = os.listdir(input_folder_openCV)\n","input_files_openCV.sort(key=natural_keys)\n","\n","input_folder_groundtruth = \"./../Data/data_masked/ground_truth_leaf/good/\"\n","input_files_groundtruth = os.listdir(input_folder_groundtruth)\n","input_files_groundtruth.sort(key=natural_keys)\n","dice_score_dict, iou_score_dict = {},{}\n","count =  0\n","for file_openCV, file_groundtruth in zip(input_files_openCV, input_files_groundtruth):\n","    print(f\"Processing {file_openCV} and {file_groundtruth}\")  # Check if loop is entered\n","\n","    pth_to_file_openCV = os.path.join(input_folder_openCV, file_openCV)\n","    pth_to_file_groundtruth = os.path.join(input_folder_groundtruth, file_groundtruth)\n","    groundtruth_green, superimposed  = color_difference(pth_to_file_openCV, pth_to_file_groundtruth, dice_score_dict,  count)\n","    save_image(image = superimposed , image_name = 'superimposed_Self_supervised_segmentation_good', count = count, Method= 'Self_supervised_segmentation', specific_folder= 'comparision_files',category='good')\n","    #save_image(image = groundtruth_green , image_name = 'groundtruth_green', count = count, Method= 'green_ground_truth', specific_folder= 'specific_folder', category='good')\n","    count += 1\n","dice_score_dict = pd.DataFrame(dice_score_dict.values(), columns=['dice_score'], index=dice_score_dict.keys())\n","with open(\"./../comparision_leaf_segmentation/Self_supervised_segmentation/comparision_files/good/dice_score_Self_supervised_segmentation_good.txt\", \"w\") as f:\n","             f.write(dice_score_dict.to_string())\n","#generating comparisions between groundtruth and OpenCV for scap images\n","input_folder_openCV = \"./../comparision_leaf_segmentation/Self_supervised_segmentation/generated_masks/scap/\"\n","input_files_openCV = os.listdir(input_folder_openCV)\n","input_files_openCV.sort(key=natural_keys)\n","\n","input_folder_groundtruth = \"./../Data/data_masked/ground_truth_leaf/scap/\"\n","input_files_groundtruth = os.listdir(input_folder_groundtruth)\n","input_files_groundtruth.sort(key=natural_keys)\n","dice_score_dict, iou_score_dict = {},{}\n","count =  0\n","for file_openCV, file_groundtruth in zip(input_files_openCV, input_files_groundtruth):\n","    print(f\"Processing {file_openCV} and {file_groundtruth}\")  # Check if loop is entered\n","\n","    pth_to_file_openCV = os.path.join(input_folder_openCV, file_openCV)\n","    pth_to_file_groundtruth = os.path.join(input_folder_groundtruth, file_groundtruth)\n","    groundtruth_green, superimposed  = color_difference(pth_to_file_openCV, pth_to_file_groundtruth, dice_score_dict,  count)\n","    save_image(image = superimposed , image_name = 'superimposed_Self_supervised_segmentation_scap', count = count, Method= 'Self_supervised_segmentation', specific_folder= 'comparision_files',category='scap')\n","    #save_image(image = groundtruth_green , image_name = 'groundtruth_green', count = count, Method= 'green_ground_truth', specific_folder= 'specific_folder', category='scap')\n","    count += 1\n","dice_score_dict = pd.DataFrame(dice_score_dict.values(), columns=['dice_score'], index=dice_score_dict.keys())\n","with open(\"./../comparision_leaf_segmentation/Self_supervised_segmentation/comparision_files/scap/dice_score_Self_supervised_segmentation_scap.txt\", \"w\") as f:\n","             f.write(dice_score_dict.to_string())\n"],"metadata":{"id":"ppnWwEugUX5j","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698414562841,"user_tz":-120,"elapsed":567352,"user":{"displayName":"CS Biostatistic PLEN","userId":"00637746287386755639"}},"outputId":"1845dee8-f5a1-465c-8795-75932d617ec0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Iterations 199/200: 15, 3.20: 100% 200/200 [00:17<00:00, 11.13it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 17, 3.20: 100% 200/200 [00:10<00:00, 19.32it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 16, 3.20: 100% 200/200 [00:10<00:00, 19.30it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 7, 3.20: 100% 200/200 [00:10<00:00, 19.18it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 9, 3.20: 100% 200/200 [00:10<00:00, 19.28it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 12, 3.20: 100% 200/200 [00:10<00:00, 19.34it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 14, 3.20: 100% 200/200 [00:10<00:00, 19.19it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 11, 3.20: 100% 200/200 [00:10<00:00, 19.22it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 17, 3.20: 100% 200/200 [00:10<00:00, 19.26it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 13, 3.20: 100% 200/200 [00:12<00:00, 15.94it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 10, 3.20: 100% 200/200 [00:10<00:00, 19.26it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 22, 3.20: 100% 200/200 [00:10<00:00, 19.42it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 8, 3.20: 100% 200/200 [00:10<00:00, 19.10it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 20, 3.21: 100% 200/200 [00:10<00:00, 19.27it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 11, 3.21: 100% 200/200 [00:10<00:00, 19.15it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 21, 3.20: 100% 200/200 [00:10<00:00, 19.15it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 18, 3.21: 100% 200/200 [00:10<00:00, 19.32it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 9, 3.20: 100% 200/200 [00:10<00:00, 19.25it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 23, 3.21: 100% 200/200 [00:10<00:00, 19.35it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 15, 3.21: 100% 200/200 [00:10<00:00, 19.19it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 15, 3.21: 100% 200/200 [00:10<00:00, 19.27it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 13, 3.21: 100% 200/200 [00:10<00:00, 19.15it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 14, 3.20: 100% 200/200 [00:10<00:00, 19.21it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 10, 3.21: 100% 200/200 [00:10<00:00, 19.17it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 14, 3.21: 100% 200/200 [00:10<00:00, 19.11it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 13, 3.21: 100% 200/200 [00:10<00:00, 19.28it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 14, 3.21: 100% 200/200 [00:10<00:00, 19.25it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 15, 3.20: 100% 200/200 [00:10<00:00, 19.26it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 24, 3.20: 100% 200/200 [00:10<00:00, 19.31it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 12, 3.20: 100% 200/200 [00:10<00:00, 19.33it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 11, 3.21: 100% 200/200 [00:10<00:00, 19.16it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 14, 3.20: 100% 200/200 [00:10<00:00, 19.07it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 17, 3.20: 100% 200/200 [00:10<00:00, 19.15it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 12, 3.20: 100% 200/200 [00:10<00:00, 19.08it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 15, 3.21: 100% 200/200 [00:10<00:00, 19.28it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 22, 3.20: 100% 200/200 [00:10<00:00, 19.25it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 21, 3.21: 100% 200/200 [00:10<00:00, 19.23it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 12, 3.20: 100% 200/200 [00:10<00:00, 19.19it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 9, 3.20: 100% 200/200 [00:10<00:00, 19.30it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 18, 3.21: 100% 200/200 [00:10<00:00, 19.30it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 18, 3.20: 100% 200/200 [00:10<00:00, 19.18it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Iterations 199/200: 9, 3.20: 100% 200/200 [00:10<00:00, 19.25it/s]\n","Figure(1500x1000)\n","Result has been saved in ./output/result.jpg\n","Processing 0generated_mask.png and 0leaf_mask.png\n","Processing 1generated_mask.png and 1leaf_mask.png\n","Processing 2generated_mask.png and 2leaf_mask.png\n","Processing 3generated_mask.png and 3leaf_mask.png\n","Processing 4generated_mask.png and 4leaf_mask.png\n","Processing 5generated_mask.png and 5leaf_mask.png\n","Processing 6generated_mask.png and 6leaf_mask.png\n","Processing 7generated_mask.png and 7leaf_mask.png\n","Processing 8generated_mask.png and 8leaf_mask.png\n","Processing 0generated_mask.png and 0leaf_mask.png\n","Processing 1generated_mask.png and 1leaf_mask.png\n","Processing 2generated_mask.png and 2leaf_mask.png\n","Processing 3generated_mask.png and 3leaf_mask.png\n","Processing 4generated_mask.png and 4leaf_mask.png\n","Processing 5generated_mask.png and 5leaf_mask.png\n","Processing 6generated_mask.png and 6leaf_mask.png\n","Processing 7generated_mask.png and 7leaf_mask.png\n","Processing 8generated_mask.png and 8leaf_mask.png\n","Processing 9generated_mask.png and 9leaf_mask.png\n","Processing 10generated_mask.png and 10leaf_mask.png\n","Processing 11generated_mask.png and 11leaf_mask.png\n","Processing 12generated_mask.png and 12leaf_mask.png\n","Processing 13generated_mask.png and 13leaf_mask.png\n","Processing 14generated_mask.png and 14leaf_mask.png\n","Processing 15generated_mask.png and 15leaf_mask.png\n","Processing 16generated_mask.png and 16leaf_mask.png\n","Processing 17generated_mask.png and 17leaf_mask.png\n","Processing 18generated_mask.png and 18leaf_mask.png\n","Processing 19generated_mask.png and 19leaf_mask.png\n","Processing 20generated_mask.png and 20leaf_mask.png\n","Processing 21generated_mask.png and 21leaf_mask.png\n","Processing 22generated_mask.png and 22leaf_mask.png\n","Processing 23generated_mask.png and 23leaf_mask.png\n","Processing 24generated_mask.png and 24leaf_mask.png\n","Processing 25generated_mask.png and 25leaf_mask.png\n","Processing 26generated_mask.png and 26leaf_mask.png\n","Processing 27generated_mask.png and 27leaf_mask.png\n","Processing 28generated_mask.png and 28leaf_mask.png\n","Processing 29generated_mask.png and 29leaf_mask.png\n","Processing 30generated_mask.png and 30leaf_mask.png\n","Processing 31generated_mask.png and 31leaf_mask.png\n","Processing 32generated_mask.png and 32leaf_mask.png\n"]}]}]}