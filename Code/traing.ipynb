{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import (\n",
    "    Callback,\n",
    "    LightningDataModule,\n",
    "    LightningModule,\n",
    "    Trainer,\n",
    "    seed_everything,\n",
    ")\n",
    "from omegaconf import DictConfig, OmegaConf, open_dict\n",
    "from pytorch_lightning.plugins import DDPPlugin\n",
    "import hydra\n",
    "from omegaconf import DictConfig\n",
    "from typing import List, Optional\n",
    "import wandb \n",
    "import os\n",
    "import warnings\n",
    "import torch\n",
    "from src.utils import utils\n",
    "from pytorch_lightning.loggers import LightningLoggerBase\n",
    "import pickle\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "os.environ['NUMEXPR_MAX_THREADS'] = '16'\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\", \".*Trying to infer the `batch_size` from an ambiguous collection.*\"\n",
    ")\n",
    "\n",
    "\n",
    "log = utils.get_logger(__name__) # init logger\n",
    "\n",
    "@hydra.main(config_path='configs', config_name='config') # Hydra decorator\n",
    "def train(cfg: DictConfig) -> Optional[float]: \n",
    "    results = {}\n",
    "\n",
    "    # base names for logging\n",
    "    base = cfg.callbacks.model_checkpoint.monitor # naming of logs\n",
    "    if 'early_stop' in cfg.callbacks:\n",
    "        base_es = cfg.callbacks.early_stop.monitor # early stop base metric\n",
    "\n",
    "    # load checkpoint if specified\n",
    "    if cfg.get('load_checkpoint')  and (cfg.get('onlyEval') or cfg.get('resume_train')): # load stored checkpoint for testing or resuming training\n",
    "        wandbID, checkpoints = utils.get_checkpoint(cfg, cfg.get('load_checkpoint')) # outputs a Dictionary of checkpoints and the corresponding wandb ID to resume the run \n",
    "        if cfg.get('new_wandb_run',False): # If we want to onlyEvaluate a run in a new wandb run\n",
    "            cfg.logger.wandb.id = wandb.util.generate_id()\n",
    "        else:\n",
    "            if cfg.get('resume_wandb',True):\n",
    "                log.info(f\"Resuming wandb run\")\n",
    "                cfg.logger.wandb.resume = wandbID # this will allow resuming the wandb Run \n",
    "\n",
    "    cfg.logger.wandb.group = cfg.name  # specify group name in wandb \n",
    "\n",
    "    # Set plugins for lightning trainer\n",
    "    if cfg.trainer.get('accelerator',None) == 'ddp': # for better performance in ddp mode\n",
    "        plugs = DDPPlugin(find_unused_parameters=False)\n",
    "    else: \n",
    "        plugs = None\n",
    "\n",
    "    if \"seed\" in cfg: # for deterministic training (covers pytorch, numpy and python.random)\n",
    "        log.info(f\"Seed specified to {cfg.seed} by config\")\n",
    "        seed_everything(cfg.seed, workers=True)\n",
    "\n",
    "    # get start and end fold\n",
    "    start_fold = cfg.get('start_fold',0)\n",
    "    end_fold = cfg.get('num_folds',5)\n",
    "    if start_fold != 0:\n",
    "        log.info(f'skipping the first {start_fold} fold(s)') \n",
    "\n",
    "    # iterate over folds from start_fold to num_fold\n",
    "    for fold in range(start_fold,end_fold): # iterate over folds \n",
    "        \n",
    "        log.info(f\"Training Fold {fold+1} of {end_fold} in the WandB group {cfg.logger.wandb.group}\")\n",
    "        prefix = f'{fold+1}/' # naming of logs\n",
    "\n",
    "\n",
    "        cfg.datamodule._target_ = f'src.datamodules.Datamodules_train.{cfg.datamodule.cfg.name}' # set datamodule target\n",
    "        log.info(f\"Instantiating datamodule <{cfg.datamodule._target_}>\") \n",
    "        datamodule_train: LightningDataModule = hydra.utils.instantiate(cfg.datamodule,fold=fold) # instantiate datamodule\n",
    "\n",
    "        # Init lightning model\n",
    "        log.info(f\"Instantiating model <{cfg.model._target_}>\")\n",
    "        model: LightningModule = hydra.utils.instantiate(cfg.model,prefix=prefix) # instantiate model\n",
    "\n",
    "        # setup callbacks\n",
    "        cfg.callbacks.model_checkpoint.monitor = f'{prefix}' + base # naming of logs for cross validation\n",
    "        cfg.callbacks.model_checkpoint.filename = \"epoch-{epoch}_step-{step}_loss-{\"+f\"{prefix}\"+\"val/loss:.2f}\" # naming of logs for cross validation\n",
    "\n",
    "        if 'early_stop' in cfg.callbacks:\n",
    "            cfg.callbacks.early_stop.monitor = f'{prefix}' + base_es # naming of logs for cross validation\n",
    "\n",
    "        if 'log_image_predictions' in cfg.callbacks:\n",
    "            cfg.callbacks.log_image_predictions.prefix = prefix # naming of logs for cross validation\n",
    "        \n",
    "        # init callbacks\n",
    "        callbacks: List[Callback] = []\n",
    "        if \"callbacks\" in cfg:\n",
    "            for _, cb_conf in cfg.callbacks.items():\n",
    "                if \"_target_\" in cb_conf:\n",
    "                    log.info(f\"Instantiating callback <{cb_conf._target_}>\")\n",
    "                    callbacks.append(hydra.utils.instantiate(cb_conf))\n",
    "            callbacks[0].FILE_EXTENSION = f'_fold-{fold+1}.ckpt' # naming of logs for cross validation callbacks[0] is the model checkpoint callback (this is a hacky way to do this)\n",
    "\n",
    "        # Init lightning loggers\n",
    "        logger: List[LightningLoggerBase] = []\n",
    "        if \"logger\" in cfg:\n",
    "            for _, lg_conf in cfg.logger.items():\n",
    "                if \"_target_\" in lg_conf:\n",
    "                    log.info(f\"Instantiating logger <{lg_conf._target_}>\")\n",
    "                    logger.append(hydra.utils.instantiate(lg_conf))\n",
    "\n",
    "        # Load checkpoint if specified\n",
    "        if cfg.get('load_checkpoint') and (cfg.get('onlyEval',False) or cfg.get('resume_train',False) ): # pass checkpoint to resume from\n",
    "            with open_dict(cfg):\n",
    "                cfg.trainer.resume_from_checkpoint = checkpoints[f\"fold-{fold+1}\"]\n",
    "                cfg.ckpt_path=None\n",
    "            log.info(f\"Restoring Trainer State of loaded checkpoint: \",cfg.trainer.resume_from_checkpoint)\n",
    "\n",
    "        # Init lightning trainer\n",
    "        log.info(f\"Instantiating trainer <{cfg.trainer._target_}>\")\n",
    "        trainer: Trainer = hydra.utils.instantiate(\n",
    "            cfg.trainer, callbacks=callbacks, logger=logger, _convert_=\"partial\", plugins=plugs\n",
    "        )          \n",
    "\n",
    "        # Send some parameters from config to all lightning loggers\n",
    "        log.info(\"Logging hyperparameters!\")\n",
    "        utils.log_hyperparameters(\n",
    "            config=cfg,\n",
    "            model=model,\n",
    "            datamodule=datamodule_train,\n",
    "            trainer=trainer,\n",
    "            callbacks=callbacks,\n",
    "            logger=logger,\n",
    "        )\n",
    "\n",
    "\n",
    "        if (not cfg.get('onlyEval',False) or cfg.get('resume_train',False)) : # train model\n",
    "            trainer.fit(model, datamodule_train)\n",
    "            validation_metrics = trainer.callback_metrics\n",
    "        else: # load trained model\n",
    "            model.load_state_dict(torch.load(checkpoints[f'fold-{fold+1}'])['state_dict'])\n",
    "\n",
    "        # logging\n",
    "        log.info(f\"Best checkpoint path:\\n{trainer.checkpoint_callback.best_model_path}\")\n",
    "        log.info(f\"Best checkpoint metric:\\n{trainer.checkpoint_callback.best_model_score}\")\n",
    "        trainer.logger.experiment[0].log({'best_ckpt_path':trainer.checkpoint_callback.best_model_path})\n",
    "        trainer.logger.experiment[0].log({'logdir':trainer.log_dir})\n",
    "\n",
    "        # metrics\n",
    "        validation_metrics = trainer.callback_metrics\n",
    "        for key in validation_metrics:\n",
    "            key =  key[2:]\n",
    "            valkey= prefix + key\n",
    "            if not 'train' in key and not 'test' in key:\n",
    "                if key not in results:\n",
    "                    results[key] = []\n",
    "                results[key].append(validation_metrics[valkey])\n",
    "\n",
    "    # Evaluate model on test set, using the best or last model from each trained fold \n",
    "\n",
    "        if cfg.get(\"test_after_training\"): # and not 'simclr' in  cfg.model._target_.lower():\n",
    "            log.info(f\"Starting evaluation phase of fold {fold+1}!\")\n",
    "            preds_dict = {}\n",
    "            preds_dict = {'val':{},'test':{}} # a dict for each data set\n",
    "            \n",
    "            sets = {\n",
    "                    't2':['Datamodules_eval.Brats21','Datamodules_eval.MSLUB','Datamodules_train.IXI'],\n",
    "                   }\n",
    "            \n",
    "                \n",
    "            for set in cfg.datamodule.cfg.testsets :\n",
    "                if not set in sets[cfg.datamodule.cfg.mode]: # skip testsets of different modalities\n",
    "                    continue    \n",
    "\n",
    "                cfg.datamodule._target_ = 'src.datamodules.{}'.format(set)\n",
    "                log.info(f\"Instantiating datamodule <{cfg.datamodule._target_}>\")\n",
    "                datamodule: LightningDataModule = hydra.utils.instantiate(cfg.datamodule, fold=fold)\n",
    "                datamodule.setup()\n",
    "\n",
    "                # Validation steps\n",
    "                log.info(\"Validation of {}!\".format(set))\n",
    "\n",
    "                ckpt_path=cfg.get('ckpt_path',None)\n",
    "\n",
    "                if 'train' in set:\n",
    "                    trainer.test(model=model,dataloaders=datamodule.val_eval_dataloader(),ckpt_path=ckpt_path)\n",
    "                else: \n",
    "                    trainer.test(model=model,dataloaders=datamodule.val_dataloader(),ckpt_path=ckpt_path)\n",
    "                # evaluation results\n",
    "                preds_dict['val'][set] = trainer.lightning_module.eval_dict\n",
    "                log_dict = utils.summarize(preds_dict['val'][set],'val') # sets prefix val/ and removes lists for better logging in wandb\n",
    "\n",
    "                # Test steps\n",
    "                log.info(\"Test of {}!\".format(set))\n",
    "                if 'train' in set:\n",
    "                    trainer.test(model=model,dataloaders=datamodule.test_eval_dataloader(),ckpt_path=ckpt_path)\n",
    "                else: \n",
    "                    trainer.test(model=model,dataloaders=datamodule.test_dataloader(),ckpt_path=ckpt_path)\n",
    "\n",
    "                # log to wandb\n",
    "                preds_dict['test'][set] = trainer.lightning_module.eval_dict\n",
    "                log_dict.update(utils.summarize(preds_dict['test'][set],'test')) # sets prefix test/ and removes lists for better logging in wandb\n",
    "                log_dict = utils.summarize(log_dict,f'{fold+1}/'+set) # sets prefix for each data set\n",
    "                trainer.logger.experiment[0].log(log_dict)\n",
    "\n",
    "                \n",
    "\n",
    "            # pickle preds_dict for later analysis\n",
    "            if cfg.get('pickle_preds',True):\n",
    "                with open(os.path.join(trainer.log_dir,f'{fold+1}_preds_dict.pkl'),'wb') as f:\n",
    "                    pickle.dump(preds_dict,f)\n",
    "\n",
    "\n",
    "\n",
    "    # Make sure everything closed properly\n",
    "    log.info(\"Finalizing!\")\n",
    "    utils.finish(\n",
    "        config=cfg,\n",
    "        model=model,\n",
    "        datamodule=datamodule_train,\n",
    "        trainer=trainer,\n",
    "        callbacks=callbacks,\n",
    "        logger=logger,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
